import urllib.requests

def find_a(s: str, start=0) -> tuple:
    start_i = s.find("<a", start)
    if start_i == -1:
        return -1, -1
    href_i = s.find('href="', start_i)
    href_start = href_i + 6
    href_finish = s.find('"', href_start)
    return href_start, href_finish

def link_to_file_name(s: str) -> str:# переименовываем ссылку в имя файла
    if s.startswith('/'):
        s = s[1:]
    s = s.replace('/', '_')
    if not s.endswith('.html'):
        s += '.html'
    return s

def links_to_files_names(a: list) -> list:#меняем все внутренние ссылки на имена файлов
    d = []
    for el in a:
        d.append(link_to_file_name(el))
    return d

def load_html(url: str) -> str:# скачиваем страницу и возвращаем текст
    result = requests.get(url)
    return result.text

def find_vnutr(text: str, url: str) -> list:#находим все внутренние ссылки
    finish_href = 0
    a = []
    while True:
        start_href, finish_href = find_a(text, finish_href)
        if finish_href == -1:
            break
        if not text[start_href:finish_href].startswith('https'):
            b = url[0:-1] + text[start_href:finish_href]
            a.append(b)
    return a


saved_links = set()

def is_saved(url):#определяем ходили ли мы уже по этой ссылке
    return url in saved_links

def set_saved(url):#помечаем ссылку как хоженую
    if not is_saved(url):
        saved_links.add(url)

def save_html(text: str, name: str) -> None:#по тексту и имени файла сохраняем текст
    with open(name, 'w', encoding='utf-8') as f:
        f.write(text)

def link_to_file_name(s:str) -> str:
    if s.startswith('/'):
        s = s[1:]
    s = s.replace('/','_')
    if not s.endswith('.html'):
        s += '.html'
    return(s)


def links_to_file_names(s: str) -> str:
    for el in s:
        if el.startswith('/'):
            s = s[1:]
        el = el.replace('/','_')
        if not s.endswith('.html'):
            s += '.html'
    return(s)

if __name__ == '__main__':
    url = "https://www.gimp.org/"
    result = requests.get(url)
